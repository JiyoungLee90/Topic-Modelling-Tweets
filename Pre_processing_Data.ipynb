{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa8d51f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "410ccb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0e416050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweet_df = pd.read_csv('Data/tweets_climate_change.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f00ae",
   "metadata": {},
   "source": [
    "## Creating Preprocessing Methods for Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a705041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyPreprocessor:\n",
    "    \n",
    "    def __init__(self, spacy_model = None, remove_nums = True,\n",
    "                remove_special = True, remove_stopwords = True, \n",
    "                lemmatise = True):\n",
    "        \n",
    "        self.remove_nums = remove_nums\n",
    "        self.remove_special = remove_special\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatise = lemmatise\n",
    "        \n",
    "        if not spacy_model:\n",
    "            self.model = spacy.load('en_core_web_sm')\n",
    "        else:\n",
    "            self.model = spacy_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_spacy_model(model = 'en_core_web_sm'):\n",
    "        print(f'Downloading spaCy model {model}')\n",
    "        spcy.cli.download(model)\n",
    "        print(f'Finished downloading model')\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_model(model = 'en_core_web_sm'):\n",
    "        \n",
    "        return spacy.load(model, disable = ['ner', 'parser'])\n",
    "  \n",
    "    \n",
    "    def tokenise(self, text) -> List[str]:\n",
    "        doc = self.model(text)\n",
    "        \n",
    "        return [token.text for token in doc]\n",
    "    \n",
    "    def preprocess_text(self, text) -> str:\n",
    "        doc = self.model(text)\n",
    "        \n",
    "        return self.clean_text(doc)\n",
    "    \n",
    "    def preprocess_text_list(self, texts = List[str]) -> List[str]:\n",
    "        texts_cleaned = []\n",
    "        for doc in tqdm(self.model.pipe(texts)):\n",
    "            texts_cleaned.append(self.clean_text(doc))\n",
    "            \n",
    "        return texts_cleaned\n",
    "    \n",
    "    def clean_text(self, doc: Doc) -> str:\n",
    "        \n",
    "        tokens = []\n",
    "        \n",
    "        # Remove numbers\n",
    "        if self.remove_nums:\n",
    "            for token in doc:\n",
    "                if not (token.like_num or token.is_currency):\n",
    "                    tokens.append(token)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if not token.is_stop]\n",
    "            \n",
    "        # Remove unwanted tokens\n",
    "        tokens = [\n",
    "            token for token in tokens\n",
    "            if not ( \n",
    "                token.is_punct or token.is_space or token.is_quote or token.is_bracket\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Remove empty tokens\n",
    "        tokens = [token for token in tokens if token.text.strip() != '']\n",
    "        \n",
    "        # Remove 'amp' tokens\n",
    "        tokens = [token for token in tokens if not token.text == 'amp']\n",
    "\n",
    "        # Lemmatise\n",
    "        if self.lemmatise:\n",
    "            text = ' '.join([token.lemma_ for token in tokens])\n",
    "        else:\n",
    "            text = ' '.join([token.text for token in tokens])\n",
    "            \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # Remove non alphabetic characters\n",
    "        if self.remove_special:\n",
    "            text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "            \n",
    "        # Remove non-Unicode characters\n",
    "        #text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "        \n",
    "        text = text.lower()\n",
    "        tokens = self.tokenise(text)\n",
    "        tokens = [token for token in tokens if token.strip() != ''] # filter out empty tokens again\n",
    "        \n",
    "        return text, tokens\n",
    "         \n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "    #spacy_model = SpacyPreprocessor.load_model()\n",
    "    #preprocessor = SpacyPreprocessor(spacy_model = spacy_model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "02d21909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67406it [10:34, 106.17it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = SpacyPreprocessor()\n",
    "cleaned_tweets = preprocessor.preprocess_text_list(tweet_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "95a3dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack the list of cleaned tweets\n",
    "cleaned_tweets_text = [item[0] for item in cleaned_tweets]\n",
    "cleaned_tweets_token = [item[1] for item in cleaned_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a04a25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['Processed_text'] = cleaned_tweets_text\n",
    "tweet_df['Processed_token'] = cleaned_tweets_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c9718770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Processed_text</th>\n",
       "      <th>Processed_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18257804</td>\n",
       "      <td>stateless</td>\n",
       "      <td>we’re pretty lucky, all things considered, whe...</td>\n",
       "      <td>pretty lucky thing consider compare place clim...</td>\n",
       "      <td>[pretty, lucky, thing, consider, compare, plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1246325069841723392</td>\n",
       "      <td>TsaiJilly</td>\n",
       "      <td>#UN75 survey found that respondents in all reg...</td>\n",
       "      <td>un   survey find respondent region identify cl...</td>\n",
       "      <td>[un, survey, find, respondent, region, identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1248988647812222978</td>\n",
       "      <td>Beatric54184322</td>\n",
       "      <td>All hat, no policy #climatechange #insiders ht...</td>\n",
       "      <td>hat policy climatechange insider</td>\n",
       "      <td>[hat, policy, climatechange, insider]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240072798</td>\n",
       "      <td>LugubriousLarry</td>\n",
       "      <td>Two great stories on #Maine this weekend: firs...</td>\n",
       "      <td>great story maine weekend important piece  jan...</td>\n",
       "      <td>[great, story, maine, weekend, important, piec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1124447266205503488</td>\n",
       "      <td>All435Reps</td>\n",
       "      <td>The evidence is right in front of us. Temperat...</td>\n",
       "      <td>evidence right temperature get hot climatechan...</td>\n",
       "      <td>[evidence, right, temperature, get, hot, clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67401</th>\n",
       "      <td>1329260903527718913</td>\n",
       "      <td>Jillian18277886</td>\n",
       "      <td>Once we get through #COVID, let's not forget w...</td>\n",
       "      <td>covid let forget go die climatechange</td>\n",
       "      <td>[covid, let, forget, go, die, climatechange]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67402</th>\n",
       "      <td>19435213</td>\n",
       "      <td>wildweatherdan</td>\n",
       "      <td>At the end of the last Ice Age, people changed...</td>\n",
       "      <td>end ice age people change clothe literally cli...</td>\n",
       "      <td>[end, ice, age, people, change, clothe, litera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67403</th>\n",
       "      <td>1401488848635502592</td>\n",
       "      <td>AnonWatchers</td>\n",
       "      <td>Coral #reefs ,rainforest of the ocean\\n🐠\\n#cli...</td>\n",
       "      <td>coral reef rainforest ocean   climatechange co...</td>\n",
       "      <td>[coral, reef, rainforest, ocean, climatechange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67404</th>\n",
       "      <td>18027211</td>\n",
       "      <td>dennissweatt</td>\n",
       "      <td>What is #climatechange? \\n\\nWets are wetter, d...</td>\n",
       "      <td>climatechange wet wetter dry dryer hot hot col...</td>\n",
       "      <td>[climatechange, wet, wetter, dry, dryer, hot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67405</th>\n",
       "      <td>854746382254706688</td>\n",
       "      <td>350Deschutes</td>\n",
       "      <td>Dirty unethical #oil lobby is killing s with i...</td>\n",
       "      <td>dirty unethical oil lobby kill s inaction clim...</td>\n",
       "      <td>[dirty, unethical, oil, lobby, kill, s, inacti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67406 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID             Name  \\\n",
       "0                 18257804        stateless   \n",
       "1      1246325069841723392        TsaiJilly   \n",
       "2      1248988647812222978  Beatric54184322   \n",
       "3                240072798  LugubriousLarry   \n",
       "4      1124447266205503488       All435Reps   \n",
       "...                    ...              ...   \n",
       "67401  1329260903527718913  Jillian18277886   \n",
       "67402             19435213   wildweatherdan   \n",
       "67403  1401488848635502592     AnonWatchers   \n",
       "67404             18027211     dennissweatt   \n",
       "67405   854746382254706688     350Deschutes   \n",
       "\n",
       "                                                   Tweet  \\\n",
       "0      we’re pretty lucky, all things considered, whe...   \n",
       "1      #UN75 survey found that respondents in all reg...   \n",
       "2      All hat, no policy #climatechange #insiders ht...   \n",
       "3      Two great stories on #Maine this weekend: firs...   \n",
       "4      The evidence is right in front of us. Temperat...   \n",
       "...                                                  ...   \n",
       "67401  Once we get through #COVID, let's not forget w...   \n",
       "67402  At the end of the last Ice Age, people changed...   \n",
       "67403  Coral #reefs ,rainforest of the ocean\\n🐠\\n#cli...   \n",
       "67404  What is #climatechange? \\n\\nWets are wetter, d...   \n",
       "67405  Dirty unethical #oil lobby is killing s with i...   \n",
       "\n",
       "                                          Processed_text  \\\n",
       "0      pretty lucky thing consider compare place clim...   \n",
       "1      un   survey find respondent region identify cl...   \n",
       "2                      hat policy climatechange insider    \n",
       "3      great story maine weekend important piece  jan...   \n",
       "4      evidence right temperature get hot climatechan...   \n",
       "...                                                  ...   \n",
       "67401              covid let forget go die climatechange   \n",
       "67402  end ice age people change clothe literally cli...   \n",
       "67403  coral reef rainforest ocean   climatechange co...   \n",
       "67404  climatechange wet wetter dry dryer hot hot col...   \n",
       "67405  dirty unethical oil lobby kill s inaction clim...   \n",
       "\n",
       "                                         Processed_token  \n",
       "0      [pretty, lucky, thing, consider, compare, plac...  \n",
       "1      [un, survey, find, respondent, region, identif...  \n",
       "2                  [hat, policy, climatechange, insider]  \n",
       "3      [great, story, maine, weekend, important, piec...  \n",
       "4      [evidence, right, temperature, get, hot, clima...  \n",
       "...                                                  ...  \n",
       "67401       [covid, let, forget, go, die, climatechange]  \n",
       "67402  [end, ice, age, people, change, clothe, litera...  \n",
       "67403  [coral, reef, rainforest, ocean, climatechange...  \n",
       "67404  [climatechange, wet, wetter, dry, dryer, hot, ...  \n",
       "67405  [dirty, unethical, oil, lobby, kill, s, inacti...  \n",
       "\n",
       "[67406 rows x 5 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13deaf09",
   "metadata": {},
   "source": [
    "## Phrase Modelling : Biagram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53ecedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count = 10, threshold = 100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold = 100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatisation(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7203580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67406it [10:12, 110.10it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = SpacyPreprocessor(lemmatise = False)\n",
    "cleaned_tweets = preprocessor.preprocess_text_list(tweet_df.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c48dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets_text = [item[0] for item in cleaned_tweets]\n",
    "cleaned_tweets_token = [item[1] for item in cleaned_tweets]\n",
    "bigram_tweets = make_bigrams(cleaned_tweets_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "392e01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tweets = lemmatisation(bigram_tweets, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7f61823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['Bigram'] = bigram_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "648349cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv('Data/tweet_climate_change_processed.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
